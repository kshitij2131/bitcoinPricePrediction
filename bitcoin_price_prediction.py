# -*- coding: utf-8 -*-
"""bitcoin_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15mc6-Y3PbjHOIZyjNpFdyL7MX4QpREz3

## data preprocessing
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from itertools import cycle
import plotly.express as px

df = pd.read_csv("/content/drive/MyDrive/PRML/bitcoin_price_bitcoin_price.2013Apr-2017Aug.csv")
df.head
df.info()

for i in range(df.shape[0]):
  if df['Volume'][i] == '-':
    df.drop(i, axis=0, inplace = True)

df = df.loc[::-1].set_index(df.index)

names = cycle(['Stock Open Price','Stock Close Price','Stock High Price','Stock Low Price'])

fig = px.line(df, x=df.Date, y=[df['Open'], df['Close'], 
                                          df['High'], df['Low']],
             labels={'Date': 'Date','value':'Stock value'})
fig.update_layout(title_text='Stock analysis chart', font_size=15, font_color='black',legend_title_text='Stock Parameters')
fig.for_each_trace(lambda t:  t.update(name = next(names)))
fig.update_xaxes(showgrid=False)
fig.update_yaxes(showgrid=False)

fig.show()

df.drop(columns = ["Date"], axis=1, inplace=True)

init = list(df['Volume'])
fin = []
for x in init :
  fin.append(float(x.replace(',','')))
df["Volume"] = fin 
init = list(df['Market Cap'])
fin = []
for x in init :
  fin.append(float(x.replace(',','')))  
df["Market Cap"] = fin

"""## train_val split (70:30)"""

df

pred = "Market Cap"
## feel free to change the value of pred to some other variable
import random
def split(f):  
 d={}
 t={}
 for x in f.columns:
  l=[] 
  if x!=pred: 
   for i in range(0, (70*f.shape[0])//100):
     l.append(f[x][i])
   d[x]=l
  else: 
   for i in range(0,  (70*f.shape[0])//100):
     l.append(f[x][i])
   t[x]=l
 y_train=pd.DataFrame(t)
 x_train=pd.DataFrame(d)
 d.clear()
 t.clear()


 for x in f.columns:
  l=[] 
  if x!=pred: 
   for i in range( (70*f.shape[0])//100,  (100*f.shape[0])//100):
     l.append(f[x][i])
   d[x]=l
  else: 
   for i in range((70*f.shape[0])//100,  (100*f.shape[0])//100):
     l.append(f[x][i])
   t[x]=l
 y_test=pd.DataFrame(t)
 x_test=pd.DataFrame(d)
 d.clear()
 t.clear()
 return (x_train, y_train, x_test, y_test)

(x_train, y_train, x_val , y_val)=split(df)
print(x_train, y_train, x_val, y_val)

norm1 = MinMaxScaler()
norm1.fit(x_train)
x_train = norm1.transform(x_train)
x_val = norm1.transform(x_val)

norm2 = MinMaxScaler()
norm2.fit(y_train)
y_train = norm2.transform(y_train)
y_val = norm2.transform(y_val)

"""## building the pipeline"""

import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib

from mlxtend.feature_selection import SequentialFeatureSelector

from sklearn.linear_model import LinearRegression
import xgboost as xgb
mod1  = DecisionTreeRegressor(criterion = "squared_error", splitter = "best", max_depth = 7, min_impurity_decrease=1e-10)
mod2  = LinearRegression()
mod3  = xgb.XGBRegressor(learning_rate = 0.1, n_estimators = 400, max_depth = 7)

pipe1 = Pipeline([
                 ('featureSelector1', SequentialFeatureSelector(mod1, k_features=2, forward=True, floating=False, scoring='r2')),
                 ('regressor',  mod1)])
pipe2 = Pipeline([
                 ('featureSelector2', SequentialFeatureSelector(mod1, k_features=2, forward=True, floating=False, scoring='r2')),
                 ('regressor',  mod2)])
pipe3 = Pipeline([
                 ('featureSelector3', SequentialFeatureSelector(mod1, k_features=2, forward=True, floating=False, scoring='r2')),
                 ('regressor',  mod3)])

models =["Decision Tree Regressor","Linear Regression", "XGboost Regressor"]
pipelines = [pipe1, pipe2, pipe3]
for x in pipelines:
  x.fit(x_train, y_train)

for i in range(3):
  print("{} gives a r2 score of :-> {}".format(models[i],r2_score(y_val, pipelines[i].predict(x_val))))

"""## comparison between different models

### decision tree regressor
"""

print("mean deviation using squared error using "+ models[2]+ ":->",np.sqrt(mean_squared_error(norm2.inverse_transform(y_val.flatten().reshape(1,-1)),  norm2.inverse_transform(pipe1.predict(x_val).reshape(1,-1)))))
print("mean deviation using absolute error using "+ models[2]+ ":->",mean_absolute_error(norm2.inverse_transform(y_val.flatten().reshape(1,-1)),  norm2.inverse_transform(pipe1.predict(x_val).reshape(1,-1))))

plt.scatter(list(range(x_val.shape[0])),norm2.inverse_transform(y_val), c="red")
plt.scatter(list(range(x_val.shape[0])),norm2.inverse_transform(pipe1.predict(x_val).reshape(-1,1)), c="blue")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(["actual prices","predicted prices"], loc = "upper left")
plt.title(models[0])

plt.plot(list(range(x_val.shape[0])),norm2.inverse_transform(y_val), c="red")
plt.plot(list(range(x_val.shape[0])),norm2.inverse_transform(pipe1.predict(x_val).reshape(-1,1)), c="blue")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(["actual prices","predicted prices"], loc = "upper left")
plt.title(models[0])

"""### linear regression model"""

print("mean deviation using squared error using "+ models[2]+ ":->",np.sqrt(mean_squared_error(norm2.inverse_transform(y_val.flatten().reshape(1,-1)),  norm2.inverse_transform(pipe2.predict(x_val).reshape(1,-1)))))
print("mean deviation using absolute error using "+ models[2]+ ":->",mean_absolute_error(norm2.inverse_transform(y_val.flatten().reshape(1,-1)),  norm2.inverse_transform(pipe2.predict(x_val).reshape(1,-1))))

plt.scatter(list(range(x_val.shape[0])),norm2.inverse_transform(y_val), c="red")
plt.scatter(list(range(x_val.shape[0])),norm2.inverse_transform(pipe2.predict(x_val).reshape(-1,1)), c="blue")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(["actual prices","predicted prices"], loc = "upper left")
plt.title(models[1])

plt.plot(list(range(x_val.shape[0])),norm2.inverse_transform(y_val), c="red")
plt.plot(list(range(x_val.shape[0])),norm2.inverse_transform(pipe2.predict(x_val).reshape(-1,1)), c="blue")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(["actual prices","predicted prices"], loc = "upper left")
plt.title(models[1])

"""### XGboost regressor"""

print("mean deviation using squared error using "+ models[2]+ ":->",np.sqrt(mean_squared_error(norm2.inverse_transform(y_val.flatten().reshape(1,-1)),  norm2.inverse_transform(pipe3.predict(x_val).reshape(1,-1)))))
print("mean deviation using absolute error using "+ models[2]+ ":->",mean_absolute_error(norm2.inverse_transform(y_val.flatten().reshape(1,-1)),  norm2.inverse_transform(pipe3.predict(x_val).reshape(1,-1))))

plt.scatter(list(range(x_val.shape[0])),norm2.inverse_transform(y_val), c="red")
plt.scatter(list(range(x_val.shape[0])),norm2.inverse_transform(pipe3.predict(x_val).reshape(-1,1)), c="blue")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(["actual prices","predicted prices"], loc = "upper left")
plt.title(models[2])

plt.plot(list(range(x_val.shape[0])),norm2.inverse_transform(y_val), c="red")
plt.plot(list(range(x_val.shape[0])),norm2.inverse_transform(pipe3.predict(x_val).reshape(-1,1)), c="blue")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(["actual prices","predicted prices"], loc = "upper left")
plt.title(models[2])

"""## Using LSTM-NN for prediction in immediate future"""

def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]   ###i=0, 0,1,2,3-----99   100 
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return np.array(dataX), np.array(dataY)

time_step = 15
train = np.hstack((x_train,y_train))
val = np.hstack((x_val,y_val))
xtrain, ytrain = create_dataset(train, time_step)
xval, yval = create_dataset(val, time_step)

xtrain.shape

xtrain= xtrain.reshape(xtrain.shape[0],xtrain.shape[1] , 1)
xval = xval.reshape(xval.shape[0],xval.shape[1] , 1)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import LSTM

lstm=Sequential()
lstm.add(LSTM(10,input_shape=(None,1),activation="relu"))
lstm.add(Dense(1))
lstm.compile(loss="mean_squared_error",optimizer="adam")

history = lstm.fit(xtrain,ytrain,validation_data=(xval,yval),epochs=350,batch_size=50,verbose=0)

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(loss))

plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend(loc=0)
plt.figure()
plt.show()

trainpred=lstm.predict(xtrain)
valpred=lstm.predict(xval)

trainpred = norm2.inverse_transform(trainpred)
valpred = norm2.inverse_transform(valpred)
ytrain = norm2.inverse_transform(ytrain.reshape(-1,1)) 
yval = norm2.inverse_transform(yval.reshape(-1,1))

print("train data RMSE: ", np.sqrt(mean_squared_error(ytrain,trainpred)))
print("train data MSE: ", mean_squared_error(ytrain,trainpred)) 
print("train data MAE: ", mean_absolute_error(ytrain,trainpred)) 
print("-------------------------------------------------------------------------------------") 
print("val data RMSE: ", np.sqrt(mean_squared_error(yval,valpred))) 
print("val data MSE: ", mean_squared_error(yval,valpred)) 
print("val data MAE: ", mean_absolute_error(yval,valpred))

print("train data r2 score:", r2_score(ytrain, trainpred))
print("val data r2 score:", r2_score(yval, valpred))

plt.figure(figsize = (15,8))
plt.plot(list(range(yval.shape[0])),yval, c = "red", label = "actual")
plt.plot(list(range(yval.shape[0])),valpred , c="blue", label = "predicted")
plt.xlabel("time")
plt.ylabel(pred +" of Bitcoin")
plt.legend(loc="best")

"""## Using Polynomial Regression for curve fitting"""

lst=[]
ind = 0
while (ind<df.shape[0]):
  lst.append(ind)
  ind+=1

from sklearn.preprocessing import PolynomialFeatures
crude = PolynomialFeatures(degree = 6, include_bias = True)
new_feature = crude.fit_transform(np.array(lst).reshape(-1, 1))
lr = LinearRegression ()
lr.fit(new_feature, df[pred])
poly_reg = lr.predict(new_feature)
plt.plot(lst, poly_reg , c="red")
plt.plot(lst, list(df[pred]))
plt.xlabel("time")
plt.ylabel("price of bitcoin")
plt.legend(["fitted price","actual price"], loc = "upper left")

print("train data RMSE: ", np.sqrt(mean_squared_error(poly_reg,df[pred])))
print("train data MSE: ", mean_squared_error(poly_reg, df[pred])) 
print("train data MAE: ", mean_absolute_error(poly_reg, df[pred]))

print("val data r2 score:", r2_score(df[pred], poly_reg))

"""## making the final predictions -

### crude prediction (only date required and using polynomial regression)
"""

import datetime
## you can enter any date of your choice
print("crude prediction (only date required) - ", "don't enter a date before 28/4/2013")
user_date=input("enter the date you want to know the bitcoin price on (in DD/MM/YYYY) - ")  
date=datetime.datetime.strptime(user_date,"%d/%m/%Y").date()  
reference = datetime.date(2013, 4, 28)
cap_pred = int(lr.predict(crude.transform(np.array((date - reference).days).reshape(1,-1))))
print("predicted " + pred + " of bitcoin : ", cap_pred)

"""### advanced prediction (date and other features, using LSTM)"""

## final prediction is done using LSTM (as it turns out to be the best model)
## please enter a date between 28-April-2013 to 31-July-2017
print("Advanced prediction (date, open, close, high, low and volume required) -")
user_date=input("enter the date you want to know the bitcoin price on (in DD/MM/YYYY) - ")  
date=datetime.datetime.strptime(user_date,"%d/%m/%Y").date()  
reference = datetime.date(2013, 4, 28)
time = (date - reference).days
print("predicted value" + pred + "of bitcoin - ", int(lstm.predict(np.array(df[pred][time]).reshape(1,-1))))

"""#### feel free to change the value of pred in case you want to predict some other variable"""